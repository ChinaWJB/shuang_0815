package com.horizon.ss
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{HBaseAdmin, Put}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapred.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.mapred.JobConf
import org.apache.spark.{HashPartitioner, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}



/**
  * Created by shuangmm on 2017/12/12
  */

object WordCountRank {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: WordCountRank<hostname> <port>")
      System.exit(1)
    }
    StreamingExamples.setStreamingLogLevels()

    val sparkConf = new SparkConf().setAppName("WordCount").setMaster("local[3]")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    //ssc.checkpoint(".")
    //hbase表名
    val tableName = "Rank_ms1"
    var rank = 0
    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream(args(0), args(1).toInt)
    val words = lines.flatMap(_.split(" "))
    val wordDstream = words.map(x => (x, 1)).reduceByKeyAndWindow((a:Int,b:Int) => (a + b),Seconds(10),Seconds(3))
    val rdd1 = wordDstream.transform(rdd => {
      rdd.partitionBy(new HashPartitioner(1)).sortBy(_._2,false).map(line => {
        rank += 1
        (rank,line)
      }).filter(_._1 <= 5)
    })
    rdd1.print()

    val rdd2 = rdd1.map(line => {
      val put = new Put(Bytes.toBytes((line._1).toString()))
      put.add(Bytes.toBytes("word"),Bytes.toBytes("word"),Bytes.toBytes(line._2._1.toString()))//word作为列名1
      put.add(Bytes.toBytes("word"),Bytes.toBytes("count"),Bytes.toBytes(line._2._2.toString()))//count作为列名2
      (new ImmutableBytesWritable, put)
    }).transform(rdd=>{
      val conf = HBaseConfiguration.create()
      conf.set("hbase.zookeeper.quorum","172.17.11.182,172.17.11.183,172.17.11.184,172.17.11.185")
      conf.set("hbase.zookeeper.property.clientPort","2181")
      val admin= new HBaseAdmin(conf)
      val jobConf = new JobConf(conf)
      jobConf.setOutputFormat(classOf[TableOutputFormat])
      jobConf.set(TableOutputFormat.OUTPUT_TABLE,tableName)
      rdd.saveAsHadoopDataset(jobConf)
      rdd
      }).map(line=> line._2.getRow()).print()

    ssc.start()
    ssc.awaitTermination()
  }

}
